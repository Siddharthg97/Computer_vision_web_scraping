{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d685a8c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: paddle in c:\\programdata\\anaconda3\\lib\\site-packages (1.0.2)\n",
      "Requirement already satisfied: paddlepaddle in c:\\programdata\\anaconda3\\lib\\site-packages (2.6.1)\n",
      "Requirement already satisfied: opt-einsum==3.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (3.3.0)\n",
      "Requirement already satisfied: httpx in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (0.27.0)\n",
      "Requirement already satisfied: astor in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (0.8.1)\n",
      "Requirement already satisfied: protobuf<=3.20.2,>=3.1.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (3.20.2)\n",
      "Requirement already satisfied: numpy>=1.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (1.21.5)\n",
      "Requirement already satisfied: Pillow in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (9.2.0)\n",
      "Requirement already satisfied: decorator in c:\\programdata\\anaconda3\\lib\\site-packages (from paddlepaddle) (5.1.1)\n",
      "Requirement already satisfied: certifi in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (2022.9.14)\n",
      "Requirement already satisfied: anyio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (3.5.0)\n",
      "Requirement already satisfied: sniffio in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (1.2.0)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (1.0.5)\n",
      "Requirement already satisfied: idna in c:\\programdata\\anaconda3\\lib\\site-packages (from httpx->paddlepaddle) (3.3)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\programdata\\anaconda3\\lib\\site-packages (from httpcore==1.*->httpx->paddlepaddle) (0.14.0)\n"
     ]
    }
   ],
   "source": [
    "# import paddle\n",
    "# from paddle import utils\n",
    "!pip install paddle --upgrade\n",
    "!pip install paddlepaddle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b716adac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting utils\n",
      "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Building wheels for collected packages: utils\n",
      "  Building wheel for utils (setup.py): started\n",
      "  Building wheel for utils (setup.py): finished with status 'done'\n",
      "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=795ec60324487b177ea79838d7d6e0601c2f8f4d46eebddbbeb28165d468e129\n",
      "  Stored in directory: c:\\users\\00147515\\appdata\\local\\pip\\cache\\wheels\\4c\\a5\\a3\\ab48e06c936b39960801612ee2767ff53764119f33d3d646e7\n",
      "Successfully built utils\n",
      "Installing collected packages: utils\n",
      "Successfully installed utils-1.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "352d418f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\fuzzywuzzy\\fuzz.py:11: UserWarning: Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning\n",
      "  warnings.warn('Using slow pure-python SequenceMatcher. Install python-Levenshtein to remove this warning')\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'paddle.utils'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\local_00147515\\Temp\\1\\ipykernel_9400\\1574967788.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mfuzzywuzzy\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mfuzz\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mitertools\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtee\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mislice\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddle_pdf\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msave_pages\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mconvert_paddle_ds\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnews_language_extraction\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0marticle_extraction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlang_detect\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnewspaper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\paddle_pdf.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddleocr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPaddleOCR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdraw_ocr\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mPIL\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageFont\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mImageDraw\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpytesseract\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddleocr\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;31m# See the License for the specific language governing permissions and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# limitations under the License.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[1;33m.\u001b[0m\u001b[0mpaddleocr\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mimportlib_metadata\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\paddleocr\\paddleocr.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mpaddle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtry_import\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m__dir__\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'paddle.utils'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import urllib3\n",
    "from fuzzywuzzy import fuzz\n",
    "from itertools import tee, islice\n",
    "from paddle_pdf import save_pages,convert_paddle_ds\n",
    "from news_language_extraction import article_extraction,lang_detect\n",
    "import newspaper\n",
    "import csv\n",
    "import json\n",
    "from ftlangdetect import detect\n",
    "from langcodes import tag_is_valid, Language\n",
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c960466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66da777a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def generate_ngrams(text, n):\n",
    "    # Function to generate n-grams from a text\n",
    "    if not text:\n",
    "        return []\n",
    "    words = text.split()\n",
    "    ngrams = zip(*[islice(seq, index, None) for index, seq in enumerate(tee(words, n))])\n",
    "    return [' '.join(ngram) for ngram in ngrams]\n",
    "\n",
    "\n",
    "def calculate_max_similarity(name, article):\n",
    "    # Calculate maximum similarity score using various fuzzy matching methods\n",
    "    if not name:  # If name is None or empty\n",
    "        return 0\n",
    "    \n",
    "    max_similarity = 0\n",
    "    if isinstance(name, str):  # Check if name is a valid string\n",
    "        name_length = len(name.split())\n",
    "        article_ngrams = generate_ngrams(article, name_length)\n",
    "        \n",
    "        for ngram in article_ngrams:\n",
    "            # Calculate similarity scores\n",
    "            ratio = fuzz.ratio(name, ngram)\n",
    "            # partial_ratio = fuzz.partial_ratio(name, ngram)\n",
    "            token_set_ratio = fuzz.token_set_ratio(name, ngram)\n",
    "            token_sort_ratio = fuzz.token_sort_ratio(name, ngram)\n",
    "            \n",
    "            # Get maximum similarity score\n",
    "            max_score = max(ratio, token_set_ratio, token_sort_ratio)\n",
    "            \n",
    "            # Update max_similarity if current score is higher\n",
    "            if max_score > max_similarity:\n",
    "                max_similarity = max_score\n",
    "    \n",
    "    return max_similarity\n",
    "\n",
    "\n",
    "def clean_wlname(wlname):\n",
    "    # Clean and preprocess wlname\n",
    "    wlname=wlname[0]\n",
    "    wlname=wlname.upper()\n",
    "    unwanted_substrings = [ 'A/L','A/P','S/O','D/O','SO','DO','AP','BINTI','BINTE','B','BTE','BT','IBNI','IBNU','ANAK','\\'','-','HAJI','HAJJAH','HAJAH','HJH','HJ','ALHAJ','HADJI','WIRA','PANGLIMA','IR','DR.','PROF.','SR','K.D.Y.T.M.','K.D.Y.M.M.','D.Y.M.M.','D.Y.T.M.','Y.T.M.','D.Y.A.M.','Y.M.M.','Y.A.M.','Y.BHG.','KDYTM','KDYMM','DYMM','DYTM','DYAM','YMM','YTM','YANG  DI PERTUAN  AGONG','DULI  YANG  MAHA  MULIA  SERI  PADUKA  BAGINDA','TIMBALAN  YANG  DI PERTUAN  AGONG','YANG DI PERTUAN BESAR','YANG  DI PERTUA  NEGERI','SULTAN','RAJA  PERMAISURI  AGONG','TUNKU  AMPUAN  BESAR','TUNKU  AMPUAN','TUNKU  PUAN  BESAR','SULTANAH','TENGKU  AMPUAN  BESAR','TENGKU  AMPUAN  MAHKOTA','TENGKU  AMPUAN','TENGKU  BESAR','TENGKU  MAHKOTA','PADUKA  BONDA  RAJA','RAJA  PERMAISURI','PERMAISURI','RAJA  MUDA','RAJA  PUAN  MUDA','RAJA  PUAN  BESAR','RAJA  DI HILIR','TENGKU  PUAN  MUDA','TENGKU  PUAN','YANG  DI-PERTUAN  MUDA','YANG  DI-PERTUAN  BESAR','TUANKU','TUNKU','TUN','RAJA','TENGKU','MEK','NIK','TUAN','WAN','PUTERI','PUTRI','AWANG','ABANG','DAYANG','CIK  PUAN  BESAR','CIK  PUAN','ENCHE  BESAR','CHE  PUAN  BESAR','SYED','SY','SAYED','SHARIFAH','SH','SYARIFAH','TAN  SRI','DATUK  SERI','DATUK  SRI','DATUK','DATOK','DATO  SRI','DATO  SERI','DATO','DATIN','PEHIN','YBHG','YAB','YB','YANG  AMAT  BERHORMAT','YANG  BERHORMAT','YANG  BERBAHAGIA']\n",
    "    \n",
    "    # Create a regular expression pattern that matches any of the unwanted substrings\n",
    "    pattern = r'\\b(?:' + '|'.join(map(re.escape, unwanted_substrings)) + r')\\b'\n",
    "    \n",
    "    # Remove the unwanted substrings from the names\n",
    "    wlname = re.sub(pattern, '', wlname).strip()\n",
    "\n",
    "    # Define the unwanted characters as a list\n",
    "    unwanted_chars = [',','~','!','#','\\$','%','\\^','&','\\*','_','-','\\+','=','\\}','\\{','\\]','\\[','\\|','\\\\\\\\',':',';','\\\"','<','>','\\?','\\.']\n",
    "    \n",
    "    # Construct the regular expression pattern to remove unwanted characters\n",
    "    unwanted_pattern = '|'.join(map(re.escape, unwanted_chars))\n",
    "    \n",
    "    # Remove unwanted characters and ensure no extra spaces are introduced\n",
    "    wlname = re.sub(unwanted_pattern, '', wlname).strip()\n",
    "    \n",
    "    # Replace specific terms in the wlname\n",
    "    replacements = {\n",
    "        'MUHAMMAD': 'MUHD', 'MUHAMMED': 'MUHD', 'MUHAMAD': 'MUHD', 'MUHAMED': 'MUHD', 'MUHD': 'MUHD',\n",
    "        'MOHAMMAD': 'MOHD', 'MOHAMMED': 'MOHD', 'MOHAMAD': 'MOHD', 'MOHAMED': 'MOHD', 'MOHD': 'MOHD',\n",
    "        'ABDUL': 'ABD', 'ABDOUL': 'ABD', 'ABDUR': 'ABD', 'ABDOL': 'ABD', 'ABDOOL': 'ABD', 'ABD': 'ABD', 'AB': 'ABD',\n",
    "        'AHMAD': 'AHMAD', 'AHMED': 'AHMAD', 'AHMET': 'AHMAD',\n",
    "        'NOOR': 'NOOR', 'NOR': 'NOOR', 'NUUR': 'NOOR', 'NUR': 'NOOR',\n",
    "        'ABDULLAH': 'ABDULLAH', 'ABDOLLAH': 'ABDULLAH',\n",
    "        'YUSOFF': 'YUSOFF', 'YUSOF': 'YUSOFF', 'YUSUFF': 'YUSOFF', 'YUSUF': 'YUSOFF'\n",
    "    }\n",
    "    \n",
    "    for term, replacement in replacements.items():\n",
    "        wlname = re.sub(term, replacement, wlname)\n",
    "    \n",
    "    # Remove accents and other diacritics\n",
    "    wlname = re.sub(\"[ÁÄÂÃ]\", \"A\", wlname)\n",
    "    wlname = re.sub(\"[Å]\", \"AA\", wlname)\n",
    "    wlname = re.sub(\"[ÉËÊÈ]\", \"E\", wlname)\n",
    "    wlname = re.sub(\"[ÍÏÎ]\", \"I\", wlname)\n",
    "    wlname = re.sub(\"[ÓÔÖÕ]\", \"O\", wlname)\n",
    "    wlname = re.sub(\"[ÚÜÙ]\", \"U\", wlname)\n",
    "    wlname = re.sub(\"[Ø]\", \"OE\", wlname)\n",
    "    wlname = re.sub(\"[Ç]\", \"C\", wlname)\n",
    "    wlname = re.sub(\"[Ñ]\", \"N\", wlname)\n",
    "    wlname = re.sub(\"[Ý]\", \"Y\", wlname)\n",
    "    wlname = re.sub(\"[^A-Z\\s ]+\", \"\", wlname).strip()\n",
    "    \n",
    "    return wlname\n",
    "\n",
    "\n",
    "def process_wlname_and_article(wlname, article):\n",
    "    # Main function to process wlname and article\n",
    "    wlname = clean_wlname(wlname)\n",
    "\n",
    "    # Extract and clean alias names from parentheses, '@', and '/'\n",
    "    alias_name2 = re.search(r'\\((.*?)\\)', wlname)\n",
    "    alias_name2 = alias_name2.group(1).strip() if alias_name2 else ''\n",
    "\n",
    "    wlname = re.sub(r'\\(.*?\\)', '', wlname).strip()\n",
    "\n",
    "    if '@' in wlname:\n",
    "        alias_name3 = wlname.split('@')[1].strip()\n",
    "        wlname = wlname.split('@')[0].strip()\n",
    "    else:\n",
    "        alias_name3 = ''\n",
    "\n",
    "    if '/' in wlname:\n",
    "        alias_name4 = wlname.split('/')[1].strip()\n",
    "        wlname = wlname.split('/')[0].strip()\n",
    "    else:\n",
    "        alias_name4 = ''\n",
    "    \n",
    "    # Calculate similarity scores for wlname and aliases\n",
    "    similarity1 = calculate_max_similarity(wlname, article)\n",
    "    similarity2 = calculate_max_similarity(alias_name2, article)\n",
    "    similarity3 = calculate_max_similarity(alias_name3, article)\n",
    "    similarity4 = calculate_max_similarity(alias_name4, article)\n",
    "    \n",
    "    # Return the maximum similarity score\n",
    "    max_similarity = max(similarity1, similarity2, similarity3, similarity4)\n",
    "    \n",
    "    return max_similarity\n",
    "\n",
    "def download_file(response,headers, save_as):\n",
    "    # response = requests.get(url, headers=headers, timeout=10)\n",
    "    with open(save_as, 'wb') as file:\n",
    "        file.write(response.content)\n",
    "        \n",
    "def similarity_list(wlname,text_extracted):\n",
    "    Name_relevance_similarity = []\n",
    "    if len(text_extracted)>5:\n",
    "        max_similarity = process_wlname_and_article(wlname, text_extracted)\n",
    "        # print(wlname,max_similarity)\n",
    "        Name_relevance_similarity.append(str(max_similarity))\n",
    "    else:\n",
    "        Name_relevance_similarity.append('0')\n",
    "    # print(Name_relevance_similarity)\n",
    "    return Name_relevance_similarity\n",
    "     \n",
    "# Example usage\n",
    "# wlname = ['HARIYANTO']\n",
    "# article = '''WEBSarawak Economy. Sarawak is one of the two Malaysian states situated in the northwest of Borneo. For decades, LNG and petroleum have been the primary sources of the â€¦'''\n",
    "\n",
    "# print(f\"Maximum similarity score: {max_similarity}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b055650",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import sys\n",
    "    parent_dir = '/app/Name_Screening/Relevance_Checking/anews_relavance'\n",
    "    headers = {\n",
    "    \"User-Agent\":\n",
    "    \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/{chrome_agent}.0.0.0 Safari/537.36 Edg/{edge_agent}.0.0.0\"\n",
    "    }\n",
    "    # df = pd.read_excel('/app/pruthvi_paddle_integ/POC_TMNS/src/Translation_scraping/links_to_re-run.xlsx')\n",
    "    df = pd.read_csv('/app/Name_Screening/Relevance_Checking/ind_sl_anews1.csv')\n",
    "    \n",
    "    '''\n",
    "    df['Extracted_Text'] = ''\n",
    "    df['Extracted_Text_1'] = ''\n",
    "    df['language_detect_1'] = ''\n",
    "    df['Extracted_Text_2'] = ''\n",
    "    df['language_detect_2'] = ''\n",
    "    df['language_detect'] = ''\n",
    "    df['Status_Code'] = ''\n",
    "    print(df.head(6))\n",
    "    \n",
    "    df['Extracted_Text'] = ''\n",
    "    df['language_detect'] = ''\n",
    "    \n",
    "    print(df.shape[0])\n",
    "    df.dropna(subset=['V_SOURCE_LINKS'], inplace = True)\n",
    "    df = df.drop_duplicates(keep='first')\n",
    "    count = df['V_INFO_SOURCE'].value_counts()\n",
    "    print(count)\n",
    "    df_pepex = df[df['V_INFO_SOURCE'] == 'PEPEX']\n",
    "    df_sanex = df[df['V_INFO_SOURCE'] == 'SANEX']\n",
    "    df_anews = df[df['V_INFO_SOURCE'] == 'ANEWS']\n",
    "    # print(df.shape[0])\n",
    "    df_pepex.to_csv('ind_sl_pepex.csv',index = False)\n",
    "    df_sanex.to_csv('ind_sl_sanex.csv',index = False)\n",
    "    df_anews.to_csv('ind_sl_anews.csv',index = False)\n",
    "    '''\n",
    "    \n",
    "    for i in range(0,df.shape[0]):\n",
    "        # print(i)\n",
    "        if type(df['V_SOURCE_LINKS'].iloc[i]) == type('str'):\n",
    "            list_articles = df['V_SOURCE_LINKS'].iloc[i].replace('\\n', '').split(' ')\n",
    "            wlname = []\n",
    "            if type(df['V_WATCHLIST_PARTY_NAME'].iloc[i]) == type('str'):\n",
    "                wlname.append(df['V_WATCHLIST_PARTY_NAME'].iloc[i])\n",
    "            else:\n",
    "                wlname = ['']\n",
    "            article_extracted_list_pkg = []\n",
    "            article_extracted_list_1 = []\n",
    "            article_extracted_list_2 = []\n",
    "            article_extracted_language_pkg = []\n",
    "            article_extracted_language_1 = []\n",
    "            article_extracted_language_2 = []\n",
    "            url_status_code = []\n",
    "            Name_relevance_similarity_1 = []\n",
    "            Name_relevance_similarity_2 = []\n",
    "            Name_relevance_similarity_pkg = []\n",
    "            max_similarity= 0\n",
    "            # print(list_articles,type(list_articles))\n",
    "            a = 0\n",
    "            for j in list_articles:\n",
    "                a = a+1\n",
    "                print(j)\n",
    "                text_extracted = ['']\n",
    "                text_extracted_1 = ['']\n",
    "                text_extracted_2 = ['']\n",
    "                language_extracted = ['']\n",
    "                \n",
    "                try:\n",
    "                    \n",
    "                    article_html1 = requests.get(j, headers=headers, timeout=10)\n",
    "                    print(article_html1.status_code)\n",
    "                    content_type = article_html1.headers.get('content-type')\n",
    "                    print(content_type)\n",
    "                    \n",
    "                    if article_html1.status_code!=200:\n",
    "                        content_type = 'text/html'\n",
    "                        print(content_type)\n",
    "                    folder_name = '{}_{}_{}'.format(df['N_PARTY_WATCHLIST_MATCH_ID'].iloc[i],df['V_INFO_SOURCE'].iloc[i],a)\n",
    "                    path_folder = os.path.join(parent_dir, folder_name)\n",
    "                    if 'text/html' in content_type:\n",
    "                        text_extracted,text_extracted_1,text_extracted_2 = article_extraction(j,article_html1)\n",
    "                        # print(text_extracted)\n",
    "                        article_extracted_list_pkg.append(text_extracted)\n",
    "                        article_extracted_list_1.append(text_extracted_1)\n",
    "                        article_extracted_list_2.append(text_extracted_2)\n",
    "                        \n",
    "                        article_extracted_language_pkg.append(lang_detect(text_extracted[0]))\n",
    "                        article_extracted_language_1.append(lang_detect(text_extracted_1[0]))\n",
    "                        article_extracted_language_2.append(lang_detect(text_extracted_2[0]))\n",
    "                        # similarity_list(wlname,text_extracted)\n",
    "                        Name_relevance_similarity_1.append(similarity_list(wlname,text_extracted_1[0]))\n",
    "                        Name_relevance_similarity_2.append(similarity_list(wlname,text_extracted_2[0]))\n",
    "                        Name_relevance_similarity_pkg.append(similarity_list(wlname,text_extracted[0]))\n",
    "                        print('############################################')\n",
    "                        # print(article_html1.status_code)\n",
    "                    elif 'image' in content_type:\n",
    "                        if not os.path.exists(path_folder):\n",
    "                            os.mkdir(path_folder)\n",
    "                        file_name = '{}_image_{}.tiff'.format(df['N_PARTY_WATCHLIST_MATCH_ID'].iloc[i],a)\n",
    "                        path_image = os.path.join(path_folder, file_name)\n",
    "                        download_file(article_html1,headers,path_image)\n",
    "                        # text_extracted = []\n",
    "                        text_extracted = save_pages(path_image)\n",
    "                        # print(text_extracted)\n",
    "                        article_extracted_list_pkg.append(text_extracted)\n",
    "                        article_extracted_list_1.append(['Image'])\n",
    "                        article_extracted_list_2.append(['Image'])\n",
    "                        article_extracted_language_pkg.append(lang_detect(text_extracted[0]))\n",
    "                        article_extracted_language_1.append(['Image'])\n",
    "                        article_extracted_language_2.append(['Image'])\n",
    "                        Name_relevance_similarity_1.append(['0'])\n",
    "                        Name_relevance_similarity_2.append(['0'])\n",
    "                        Name_relevance_similarity_pkg.append(similarity_list(wlname,text_extracted[0]))\n",
    "                    elif 'pdf' in content_type:\n",
    "                        if not os.path.exists(path_folder):\n",
    "                            os.mkdir(path_folder)\n",
    "                        file_name = '{}_pdf_{}.pdf'.format(df['N_PARTY_WATCHLIST_MATCH_ID'].iloc[i],a)\n",
    "                        path_pdf = os.path.join(path_folder, file_name)\n",
    "                        # print(path_pdf)\n",
    "                        download_file(article_html1,headers,path_pdf)\n",
    "                        # text_extracted = []\n",
    "                        text_extracted = save_pages(path_pdf)\n",
    "                        # text_extracted = save_pages(path_pdf)\n",
    "                        # print(text_extracted)\n",
    "                        # text_extracted(article_html1)\n",
    "                        article_extracted_list_pkg.append(text_extracted)\n",
    "                        article_extracted_list_1.append(['pdf'])\n",
    "                        article_extracted_list_2.append(['pdf'])\n",
    "                        article_extracted_language_pkg.append(lang_detect(text_extracted[0]))\n",
    "                        article_extracted_language_1.append(['pdf'])\n",
    "                        article_extracted_language_2.append(['pdf'])\n",
    "                        Name_relevance_similarity_1.append(['0'])\n",
    "                        Name_relevance_similarity_2.append(['0'])\n",
    "                        Name_relevance_similarity_pkg.append(similarity_list(wlname,text_extracted[0]))\n",
    "                    url_status_code.append(str(article_html1.status_code))\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print('********************************************')\n",
    "                    article_extracted_list_pkg.append(text_extracted)\n",
    "                    article_extracted_language_pkg.append(language_extracted)\n",
    "                    article_extracted_list_1.append(text_extracted_1)\n",
    "                    article_extracted_list_2.append(text_extracted_2)\n",
    "                    article_extracted_language_1.append(language_extracted)\n",
    "                    article_extracted_language_2.append(language_extracted)\n",
    "                    Name_relevance_similarity_1.append(['0'])\n",
    "                    Name_relevance_similarity_2.append(['0'])\n",
    "                    Name_relevance_similarity_pkg.append(['0'])\n",
    "                    url_status_code.append('')\n",
    "    \n",
    "            # df['Extracted_Text'].iloc[i]\n",
    "            # print(article_extracted_list_pkg)\n",
    "            # print(df.dtypes)\n",
    "            for column in df.columns:\n",
    "                if df[column].dtype == 'float64':\n",
    "                    df[column] = df[column].astype('object')\n",
    "            # print(df.dtypes)\n",
    "            df.at[i,'Extracted_Text_pkg'] = article_extracted_list_pkg\n",
    "    \n",
    "            df.at[i,'language_detect_pkg'] = article_extracted_language_pkg\n",
    "            df.at[i,'Name_relevance_similarity_pkg'] = Name_relevance_similarity_pkg\n",
    "            df.at[i,'Name_relevance_similarity_1'] = Name_relevance_similarity_1\n",
    "            df.at[i,'Name_relevance_similarity_2'] = Name_relevance_similarity_2\n",
    "            \n",
    "            # print(url_status_code)\n",
    "            # print(df.dtypes)\n",
    "            \n",
    "            # print(df.dtypes)\n",
    "            df['Status_Code'] = df['Status_Code'].astype('object')\n",
    "            # df.set_value(i, 'Status_Code', url_status_code)\n",
    "            df.at[i,'Status_Code'] = url_status_code\n",
    "            df.at[i,'Extracted_Text_1'] = article_extracted_list_1\n",
    "            df.at[i,'language_detect_1'] = article_extracted_language_1\n",
    "            df.at[i,'Extracted_Text_2'] = article_extracted_list_2\n",
    "            df.at[i,'language_detect_2'] = article_extracted_language_2\n",
    "            \n",
    "            # print(i)\n",
    "            if (i%500)==0:\n",
    "                df.to_csv('/app/Name_Screening/Relevance_Checking/anews_relavance/ind_sl_anews1_language.csv',index=False)\n",
    "        else:\n",
    "            pass\n",
    "    \n",
    "    df.to_csv('/app/Name_Screening/Relevance_Checking/anews_relavance/ind_sl_anews1_language.csv',index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
